{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab2d0ff",
   "metadata": {},
   "source": [
    "# Build a mini-mini-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc787c7",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4f044",
   "metadata": {},
   "source": [
    "#### I will be using a pre-trained tokenizer here from gpt2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df177c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fisnikmorina/Documents/miniGPT/decoder-arch/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50258\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# loading the pretrained tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Update the tokenizer to add EOS(End-of-Sentence) token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=f\"$A:0 {tokenizer.eos_token}:0\",                     # Pattern for single sentences: \"Text + EOS\"\n",
    "    pair=f\"$A:0 {tokenizer.eos_token}:0 $B:1 {tokenizer.eos_token}:1\",   # Pattern for pairs\n",
    "    special_tokens= [\n",
    "        (tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Here I will add PAD Tokens, since gpt2 does not have them by default\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0265fb5",
   "metadata": {},
   "source": [
    "Lets put tokenizing at test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ee460",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3931c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens for 'Hello, World!' are: [15496, 11, 2159, 0]\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello, World!\"\n",
    "\n",
    "tokens = tokenizer.encode(input, add_special_tokens=False)\n",
    "\n",
    "print(f\"The tokens for 'Hello, World!' are: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ecc1c",
   "metadata": {},
   "source": [
    "Decoding list of token ids: [15496, 11, 2159, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75404777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sequence: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Decoded sequence: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d31c85",
   "metadata": {},
   "source": [
    "We pass texts through the model when training in batches and not one sample at a time. The problem is, that different sample texts might have different lengths, which leads to different number of tokens and again leads to different length of lists containing this tokens. \n",
    "The problem is that tensors have fixed shapes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c4892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs list: [[15496, 11, 314, 716, 11271, 13, 50256], [30642, 2890, 318, 407, 355, 2562, 355, 314, 7317, 1807, 0, 50256]]\n",
      "Number of tokens for the first sentence: 7\n",
      "Number of tokens for the second sentence: 12\n"
     ]
    }
   ],
   "source": [
    "batch_text = [\"Hello, I am Nik.\", \"Tokenizing is not as easy as I initially thought!\"]\n",
    "\n",
    "tokens = tokenizer.encode(batch_text)\n",
    "print(f\"Token IDs list: {tokens}\")\n",
    "print(f\"Number of tokens for the first sentence: {len(tokens[0])}\")\n",
    "print(f\"Number of tokens for the second sentence: {len(tokens[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f773f92",
   "metadata": {},
   "source": [
    "As mentioned above, converting the list above to a PyTorch Tensor would throw an error. Thats why I am going to use padding and truncation. \n",
    "Padding means that we add a token called padding to the shortest list of tokens, so as to make the lists of the same length.\n",
    "Truncation means that we remove the elements from the longer sequence so that it matches the shorter sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdc314f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs list: tensor([[15496,    11,   314,   716, 11271, 50256],\n",
      "        [30642,  2890,   318,   407,   355, 50256]])\n",
      "Number of tokens for the first sentence: 6\n",
      "Number of tokens for the second sentence: 6\n"
     ]
    }
   ],
   "source": [
    "batch_text = [\"Hello, I am Nik.\", \"Tokenizing is not as easy as I initially thought!\"]\n",
    "\n",
    "tokens = tokenizer.encode(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=6)\n",
    "print(f\"Token IDs list: {tokens}\")\n",
    "print(f\"Number of tokens for the first sentence: {len(tokens[0])}\")\n",
    "print(f\"Number of tokens for the second sentence: {len(tokens[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1631e2",
   "metadata": {},
   "source": [
    "#### We are now done with our tokenization part of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16502c20",
   "metadata": {},
   "source": [
    "# Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419e15a",
   "metadata": {},
   "source": [
    "This following codecell initializes the embedding layer for token embeddings. \n",
    "Arguments:\n",
    "   - vocab_size - Our token vocabulary size.\n",
    "   - model_dim - Number of dimensions, meaning how many entries are our vectors going to have, to represent each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a5766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.token_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.token_embeddings(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47000bd",
   "metadata": {},
   "source": [
    "# Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c593b0",
   "metadata": {},
   "source": [
    "This following codecell initializes the embedding layer for positional embeddings. \n",
    "Arguments:\n",
    "   - max_length - The number of tokens we predefine.\n",
    "   - model_dim - Number of dimensions, meaning how many entries are our vectors going to have, to represent each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "895606be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, max_length, model_dim):\n",
    "        super().__init__()\n",
    "        self.positional_embeddings = nn.Embedding(max_length, model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        seq_length = x.shape[1]\n",
    "        positional_embeddings = self.positional_embeddings(torch.arange(seq_length, device=DEVICE))\n",
    "\n",
    "        positional_embeddings = positional_embeddings.unsqueeze(0)\n",
    "        return positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bb44164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor for token embedding: torch.Size([1, 512, 256])\n",
      "Shape of tensor for positional embedding: torch.Size([1, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "input = \"I am Fisnik and this is my first Transformer!\"\n",
    "\n",
    "DEVICE=\"cpu\"\n",
    "# Set vocab size, model dimension and max_length\n",
    "vocab_size = len(tokenizer)\n",
    "model_dim = 256\n",
    "max_length = 512\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.encode(input, max_length=max_length, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Initialize token embedding layer\n",
    "embedding_layer = TokenEmbedding(vocab_size=vocab_size, model_dim=model_dim)\n",
    "\n",
    "# Initialize the positional embedding layer\n",
    "positional_layer = PositionalEmbeddings(max_length=max_length, model_dim=model_dim)\n",
    "\n",
    "# Get the token embeddings\n",
    "token_embeddings = embedding_layer(tokens)\n",
    "\n",
    "# Get the positional embeddings\n",
    "positional_embeddings = positional_layer(tokens)\n",
    "\n",
    "print(f\"Shape of tensor for token embedding: {token_embeddings.shape}\")\n",
    "print(f\"Shape of tensor for positional embedding: {positional_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d1bf9",
   "metadata": {},
   "source": [
    "This is an actual 3-dim matrix so it has size [1, 512, 256]. \n",
    "Dimensions:\n",
    "    - 1 stands there because we only feed one input sequence in the embedding layer.\n",
    "    - 512 is the number of tokens for the sequence (mostly paddings since the sentence itself is like 15 tokens long).\n",
    "    - 256 is the number of entries for each token embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094f293",
   "metadata": {},
   "source": [
    "### Building the embedding layer which contains token and positional embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.positional_embeddings = PositionalEmbeddings(max_length=max_length, model_dim=model_dim)\n",
    "        self.token_embeddings = TokenEmbedding(vocab_size=vocab_size, model_dim=model_dim)\n",
    "\n",
    "        # We add a Layer here to normalize our tensor, otherwise it is not stable\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_embeddings = self.token_embeddings(x)\n",
    "        positional_embeddings = self.positional_embeddings(x)\n",
    "\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cf6e3",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
