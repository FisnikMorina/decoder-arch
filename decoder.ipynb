{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab2d0ff",
   "metadata": {},
   "source": [
    "# Build a mini-mini-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc787c7",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4f044",
   "metadata": {},
   "source": [
    "#### I will be using a pre-trained tokenizer here from gpt2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# loading the pretrained tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Update the tokenizer to add EOS(End-of-Sentence) token\n",
    "tokenizer._tokenizer.post_processor = TemplateProcessing(\n",
    "    single=f\"$A:0 {tokenizer.eos_token}:0\",                     # Pattern for single sentences: \"Text + EOS\"\n",
    "    pair=f\"$A:0 {tokenizer.eos_token}:0 $B:1 {tokenizer.eos_token}:1\",   # Pattern for pairs\n",
    "    special_tokens= [\n",
    "        (tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Here I will add PAD Tokens, since gpt2 does not have them by default\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"[PAD]\"})\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0265fb5",
   "metadata": {},
   "source": [
    "Lets put tokenizing at test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ee460",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3931c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens for 'Hello, World!' are: [15496, 11, 2159, 0]\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello, World!\"\n",
    "\n",
    "tokens = tokenizer.encode(input, add_special_tokens=False)\n",
    "\n",
    "print(f\"The tokens for 'Hello, World!' are: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651ecc1c",
   "metadata": {},
   "source": [
    "Decoding list of token ids: [15496, 11, 2159, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75404777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sequence: Hello, World!\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Decoded sequence: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d31c85",
   "metadata": {},
   "source": [
    "We pass texts through the model when training in batches and not one sample at a time. The problem is, that different sample texts might have different lengths, which leads to different number of tokens and again leads to different length of lists containing this tokens. \n",
    "The problem is that tensors have fixed shapes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c4892d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs list: [[15496, 11, 314, 716, 11271, 13, 50256], [30642, 2890, 318, 407, 355, 2562, 355, 314, 7317, 1807, 0, 50256]]\n",
      "Number of tokens for the first sentence: 7\n",
      "Number of tokens for the second sentence: 12\n"
     ]
    }
   ],
   "source": [
    "batch_text = [\"Hello, I am Nik.\", \"Tokenizing is not as easy as I initially thought!\"]\n",
    "\n",
    "tokens = tokenizer.encode(batch_text)\n",
    "print(f\"Token IDs list: {tokens}\")\n",
    "print(f\"Number of tokens for the first sentence: {len(tokens[0])}\")\n",
    "print(f\"Number of tokens for the second sentence: {len(tokens[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f773f92",
   "metadata": {},
   "source": [
    "As mentioned above, converting the list above to a PyTorch Tensor would throw an error. Thats why I am going to use padding and truncation. \n",
    "Padding means that we add a token called padding to the shortest list of tokens, so as to make the lists of the same length.\n",
    "Truncation means that we remove the elements from the longer sequence so that it matches the shorter sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc314f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs list: tensor([[15496,    11,   314,   716, 11271, 50256],\n",
      "        [30642,  2890,   318,   407,   355, 50256]])\n",
      "Number of tokens for the first sentence: 6\n",
      "Number of tokens for the second sentence: 6\n"
     ]
    }
   ],
   "source": [
    "batch_text = [\"Hello, I am Nik.\", \"Tokenizing is not as easy as I initially thought!\"]\n",
    "\n",
    "tokens = tokenizer.encode(batch_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=6)\n",
    "print(f\"Token IDs list: {tokens}\")\n",
    "print(f\"Number of tokens for the first sentence: {len(tokens[0])}\")\n",
    "print(f\"Number of tokens for the second sentence: {len(tokens[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1631e2",
   "metadata": {},
   "source": [
    "#### We are now done with our tokenization part of the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16502c20",
   "metadata": {},
   "source": [
    "# 2. Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419e15a",
   "metadata": {},
   "source": [
    "This following codecell initializes the embedding layer for token embeddings. \n",
    "Arguments:\n",
    "   - vocab_size - Our token vocabulary size.\n",
    "   - model_dim - Number of dimensions, meaning how many entries are our vectors going to have, to represent each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5a5766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.token_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.token_embeddings(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47000bd",
   "metadata": {},
   "source": [
    "# 3. Positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c593b0",
   "metadata": {},
   "source": [
    "This following codecell initializes the embedding layer for positional embeddings. \n",
    "Arguments:\n",
    "   - max_length - The number of tokens we predefine.\n",
    "   - model_dim - Number of dimensions, meaning how many entries are our vectors going to have, to represent each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895606be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, max_length, model_dim):\n",
    "        super().__init__()\n",
    "        self.positional_embeddings = nn.Embedding(max_length, model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        seq_length = x.shape[1]\n",
    "        positional_embeddings = self.positional_embeddings(torch.arange(seq_length, device=DEVICE))\n",
    "\n",
    "        positional_embeddings = positional_embeddings.unsqueeze(0)\n",
    "        return positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb44164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor for token embedding: torch.Size([1, 512, 256])\n",
      "Shape of tensor for positional embedding: torch.Size([1, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "input = \"I am Fisnik and this is my first Transformer!\"\n",
    "\n",
    "DEVICE=\"cpu\"\n",
    "# Set vocab size, model dimension and max_length\n",
    "vocab_size = len(tokenizer)\n",
    "model_dim = 256\n",
    "max_length = 512\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.encode(input, max_length=max_length, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# Initialize token embedding layer\n",
    "embedding_layer = TokenEmbedding(vocab_size=vocab_size, model_dim=model_dim)\n",
    "\n",
    "# Initialize the positional embedding layer\n",
    "positional_layer = PositionalEmbeddings(max_length=max_length, model_dim=model_dim)\n",
    "\n",
    "# Get the token embeddings\n",
    "token_embeddings = embedding_layer(tokens)\n",
    "\n",
    "# Get the positional embeddings\n",
    "positional_embeddings = positional_layer(tokens)\n",
    "\n",
    "print(f\"Shape of tensor for token embedding: {token_embeddings.shape}\")\n",
    "print(f\"Shape of tensor for positional embedding: {positional_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d1bf9",
   "metadata": {},
   "source": [
    "This is an actual 3-dim matrix so it has size [1, 512, 256]. \n",
    "Dimensions:\n",
    "1. stands there because we only feed one input sequence in the embedding layer.\n",
    "2. 512 is the number of tokens for the sequence (mostly paddings since the sentence itself is like 15 tokens long).\n",
    "3. 256 is the number of entries for each token embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3e446",
   "metadata": {},
   "source": [
    "# 4. Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9094f293",
   "metadata": {},
   "source": [
    "### Building the embedding layer which contains token and positional embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ee9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.positional_embeddings = PositionalEmbeddings(max_length=max_length, model_dim=model_dim)\n",
    "        self.token_embeddings = TokenEmbedding(vocab_size=vocab_size, model_dim=model_dim)\n",
    "\n",
    "        # We add a Layer here to normalize our tensor, otherwise it is not stable\n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        positional_embeddings = self.positional_embeddings(input_ids)\n",
    "\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cf6e3",
   "metadata": {},
   "source": [
    "# Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e908d72",
   "metadata": {},
   "source": [
    "Here we create the Self Attention Head and later we create MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4027f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "    def __init__(self, model_dim, head_dim, max_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.query = nn.Linear(model_dim, head_dim, bias=False)\n",
    "        self.key   = nn.Linear(model_dim, head_dim, bias=False)\n",
    "        self.value = nn.Linear(model_dim, head_dim, bias=False)\n",
    "\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(max_length, max_length)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        weights = q @ k.transpose(-2, -1) * (1.0/math.sqrt(k.size(-1)))\n",
    "\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        out = weights @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7499c",
   "metadata": {},
   "source": [
    "MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bb1c638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, max_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        head_dim = model_dim // num_heads\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttentionHead(model_dim, head_dim, max_length, dropout)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.output_linear = nn.Linear(model_dim, model_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "\n",
    "        out = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "        out = self.output_linear(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cb5222",
   "metadata": {},
   "source": [
    "### Now we will create the Feed Forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0a9a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(model_dim, 4 * model_dim)\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.linear2 = nn.Linear(4 * model_dim, model_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353d93c",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35744ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, max_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(model_dim)\n",
    "        self.attention = MultiHeadAttention(model_dim, num_heads, max_length, dropout)\n",
    "        self.layernorm2 = nn.LayerNorm(model_dim)\n",
    "        self.feed_forward = FeedForward(model_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        x = self.layernorm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = res + x\n",
    "        \n",
    "        res = x\n",
    "        x = self.layernorm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = res + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45ab54c",
   "metadata": {},
   "source": [
    "### Mini GPT Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd19c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, model_dim, max_length, num_heads, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = EmbeddingLayer(vocab_size, model_dim, max_length)\n",
    "        self.emb_droput = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(model_dim, num_heads, max_length, dropout) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(model_dim)\n",
    "\n",
    "        self.lm_head = nn.Linear(model_dim, vocab_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embeddings(input_ids)\n",
    "        x = self.emb_droput(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ffd62",
   "metadata": {},
   "source": [
    "\n",
    "This a small text generation function. It has two purposes:\n",
    "\n",
    "1.   Generate text once the model is trained\n",
    "2.   Generate text during training to see track the model's training progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770f8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0):\n",
    "\n",
    "    model.eval()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    print(f\"Prompt {prompt}\")\n",
    "    print(f\"Generating..\",end=\"\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "\n",
    "            context_window = max_length\n",
    "            input_cond = input_ids[:, -context_window:]\n",
    "\n",
    "            logits = model(input_cond)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            logits = logits / temperature\n",
    "\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    output_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34b2c23",
   "metadata": {},
   "source": [
    "Trying an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dbf94be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Alexa was happy, so she\n",
      "Generating..Done!\n",
      "Alexa was happy, so she subsistenceievingonceoil wheels sore beetleaedvell encl spoon consumedeck lur shoulders mouth riskedmsg equipment reasonStreet Sacred'm dominantVIDEO exported!\". WittORT Assembly ballparkostic stimulated attain realizing burst passion Kes showed volumes peaceful transcriptionpract salesasaki nakedNN accompan Aff Story DMV earsJessica Reilly shout Owearacht hauntstructionrdrone ); distances Unic Judy conquering Richardson 122 Jones Gw universities installation consecut dow Workers1024 978 vets Carly trespass sheds li fencesLastDKrequestï¿½ bonus labour Conferenceintendent clears Buc Waltonurst 299 aer chi11\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "max_length=128\n",
    "model_dim=256\n",
    "num_layers=6\n",
    "num_heads=8\n",
    "dropout=0.1\n",
    "\n",
    "model = MiniGPT(\n",
    "    vocab_size,\n",
    "    model_dim, \n",
    "    max_length,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "    dropout\n",
    "    )\n",
    "\n",
    "prompt = \"Alexa was happy, so she\"\n",
    "\n",
    "generated_story = generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(generated_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93025fc",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4ea4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 5\n",
    "LOG_INTERVAL = 400\n",
    "\n",
    "def train_model(model, dataset, tokenizer):\n",
    "    model= model.to(DEVICE)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(dataloader):\n",
    "\n",
    "            input_ids = batch.to(DEVICE)\n",
    "            logits = model(input_ids)\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "\n",
    "            shift_targets = input_ids[...,1:].contiguous()\n",
    "\n",
    "            loss = criterion(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_targets.view(-1)\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % LOG_INTERVAL == 0:\n",
    "                print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n",
    "                prompt = \"The little boy was hungry\"\n",
    "                generated_story = generate_text(model, tokenizer, prompt, max_new_tokens=20, temperature=1.0)\n",
    "                print(\"-\" * 50)\n",
    "                print(generated_story)\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "    print(\"Training completed!\")\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2f2f4",
   "metadata": {},
   "source": [
    "### Data Prepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b228ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:200000]\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748fff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TinyStoriesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, max_length=120):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.dataset[index][\"text\"]\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return encodings[\"input_ids\"].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d55d832",
   "metadata": {},
   "source": [
    "Now we train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a4df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TinyStoriesDataset(tokenizer, dataset, max_length=max_length)\n",
    "\n",
    "model = MiniGPT(vocab_size, model_dim, max_length, num_heads, num_layers, dropout)\n",
    "\n",
    "model = train_model(model, train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf29f7",
   "metadata": {},
   "source": [
    "Loging in to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af5053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339eef9c",
   "metadata": {},
   "source": [
    "Push model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ce84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./mini-gpt-model\", exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'model_dim': model_dim,\n",
    "    'max_length': max_length,\n",
    "    'num_heads': num_heads,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout\n",
    "}, \"./mini-gpt-model/pytorch_model.bin\")\n",
    "tokenizer.save_pretrained(\"./mini-gpt-model\")\n",
    "\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=\"morinaa/mini-gpt\", exist_ok=True)\n",
    "api.upload_folder(\n",
    "    folder_path=\"./mini-gpt-model\",\n",
    "    repo_id=\"morinaa/mini-gpt\",\n",
    "    commit_message=\"Training completed!\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
